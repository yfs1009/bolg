"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[80],{6262:(t,e)=>{e.A=(t,e)=>{const n=t.__vccOpts||t;for(const[t,o]of e)n[t]=o;return n}},2164:(t,e,n)=>{n.r(e),n.d(e,{comp:()=>r,data:()=>i});var o=n(641);const a={},r=(0,n(6262).A)(a,[["render",function(t,e){return(0,o.uX)(),(0,o.CE)("div",null,e[0]||(e[0]=[(0,o.Lk)("h1",{id:"webaudio",tabindex:"-1"},[(0,o.Lk)("a",{class:"header-anchor",href:"#webaudio"},[(0,o.Lk)("span",null,"WebAudio")])],-1),(0,o.Lk)("h2",{id:"webaudio-实时检测声音",tabindex:"-1"},[(0,o.Lk)("a",{class:"header-anchor",href:"#webaudio-实时检测声音"},[(0,o.Lk)("span",null,"WebAudio 实时检测声音")])],-1),(0,o.Lk)("div",null,[(0,o.Lk)("br"),(0,o.Lk)("br"),(0,o.eW)("代码示例：")],-1),(0,o.Lk)("pre",null,[(0,o.eW)("    detection() {\n\t\t\tvar that = this\n\t\t\t// 获取音频流\n\t\t\tnavigator.mediaDevices.getUserMedia({\n\t\t\t\t\taudio: true\n\t\t\t\t})\n\t\t\t\t.then(stream => {\n\t\t\t\t\tconst audioContext = new AudioContext();\n\t\t\t\t\tconst audioSource = audioContext.createMediaStreamSource(stream);\n\t\t\t\t\tconst mediaRecorder = new MediaRecorder(stream);\n"),(0,o.Lk)("pre",null,[(0,o.Lk)("code",null,'\t\t\t\tmediaRecorder.mimeType = "audio/wav";\n\t\t\t\t// 创建一个BiquadFilterNode，用于降低噪声\n\t\t\t\tlet lowPassFilter = audioContext.createBiquadFilter();\n\t\t\t\tlowPassFilter.type = \'lowpass\'; // 设置为低通滤波器\n\t\t\t\tlowPassFilter.frequency.value = 3000; // 设置滤波器滤除的频率范围\n\t\t\t\t// 定义一个数组来存储音频数据块\n\t\t\t\tvar audioChunks = [];\n\t\t\t\t// 创建分析器节点\n\t\t\t\tconst analyzerNode = audioContext.createAnalyser();\n\t\t\t\tanalyzerNode.fftSize = 2048;\n\t\t\t\t// 建立连接\n\t\t\t\taudioSource.connect(lowPassFilter);\n\t\t\t\t//连接扬声器（需要关闭不然会有回放）\n\t\t\t\t// lowPassFilter.connect(audioContext.destination);\n\t\t\t\t// 连接音频源到分析器节点\n\t\t\t\taudioSource.connect(analyzerNode);\n\t\t\t\t\n\n\n\t\t\t\tmediaRecorder.addEventListener(\'dataavailable\', function(event) {\n\t\t\t\t\t// 这里是录音的数据Blob对象，可以发送给后端\n\t\t\t\t\taudioChunks.push(event.data);\n\t\t\t\t\t// 使用WebSocket、Fetch API或其他方式发送audioBlob给后端\n\t\t\t\t});\n\t\t\t\t// 停止录音后，合并所有Blob对象\n\t\t\t\tmediaRecorder.addEventListener(\'stop\', function() {\n\t\t\t\t\tif (that.demostate == true) return\n\t\t\t\t\tvar combinedBlob = new Blob(audioChunks, {\n\t\t\t\t\t\ttype: \'audio/wav\'\n\t\t\t\t\t});\n\t\t\t\t\taudioChunks = []\n\n\t\t\t\t\tconsole.log(combinedBlob, "combinedBlob")\n\t\t\t\t\tvar formData = new FormData()\n\t\t\t\t\t// 此处获取到blob对象后需要设置fileName满足当前项目上传需求，其它项目可直接传把blob作为file塞入formData\n\t\t\t\t\tvar fileOfBlob = new File([combinedBlob], new Date().getTime() + \'.wav\')\n\n\t\t\t\t\tconsole.log(fileOfBlob, "fileOfBlob")\n\t\t\t\t\tformData.append(\'file\', fileOfBlob)\n\t\t\t\t\tconsole.log(formData, "formData")\n\t\t\t\t\tdocument.getElementById("loadtxt").innerText = "思考中..."\n\t\t\t\t\t// 直接用ajax上传\n\t\t\t\t\tvar xhr = new XMLHttpRequest();\n\t\t\t\t\txhr.open("POST", app.voicebase +\n\t\t\t\t\t"getWordfromWav"); /\n\t\t\t\t\txhr.onreadystatechange = function() {\n\t\t\t\t\t\tconsole.log(xhr, "响应数据")\n\t\t\t\t\t\tif (xhr.readyState == 4) {\n\t\t\t\t\t\t\tlet responseText = JSON.parse(xhr.responseText);\n\t\t\t\t\t\t\tdocument.getElementById("loadtxt").innerText = ""\n\n\t\t\t\t\t\t\tif (responseText.voice_text.result[0] != "") {\n\t\t\t\t\t\t\t\tdocument.getElementById("loadtxt").innerText = responseText\n\t\t\t\t\t\t\t\t\t.voice_text.result[0] || \'\'\n\t\t\t\t\t\t\t\tthat.问题 = responseText.voice_text.result[0]\n\t\t\t\t\t\t\t\t提交()\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\txhr.send(formData);\n\t\t\t\t\txhr.onerror = function() {\n\t\t\t\t\t\talert("上传失败");\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\t// 定义检测函数\n\t\t\t\tfunction detectSpeech() {\n\t\t\t\t\tconst bufferLength = analyzerNode.frequencyBinCount;\n\t\t\t\t\tconst dataArray = new Uint8Array(bufferLength);\n\n\t\t\t\t\tanalyzerNode.getByteFrequencyData(dataArray);\n\n\t\t\t\t\t// 检查音频数据中是否有能量值大于阈值的部分\n\t\t\t\t\tconst threshold = 120; // 调整阈值以适应实际情况\n\t\t\t\t\tvar state = false\n\t\t\t\t\tfor (let i = 0; i < bufferLength; i++) {\n\t\t\t\t\t\tif (dataArray[i] > threshold) {\n\t\t\t\t\t\t\tconsole.log("有人在说话！");\n\t\t\t\t\t\t\tstate = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif (state) {\n\t\t\t\t\t\treturn true\n\t\t\t\t\t}\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t\t// 设置一个定时器，用于检测是否有人说话\n\t\t\t\tsetInterval(() => {\n\t\t\t\t\t// 在这里可以添加检测声音的逻辑，例如使用Web Audio API分析音频数据\n\t\t\t\t\t// 如果检测到有人说话，则继续录音；否则，停止录音并发送数据给后端\n\t\t\t\t\tconst isSpeaking = detectSpeech(); // 自定义函数，返回true表示有人在说话，false表示没有人在说话\n\t\t\t\t\tif (isSpeaking) {\n\t\t\t\t\t\t// 继续录音\n\t\t\t\t\t\tif (that.demostate == true) {\n\t\t\t\t\t\t\tdocument.getElementById("loadtxt").innerText = ""\n\t\t\t\t\t\t\tmediaRecorder.stop();\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\tdocument.getElementById("loadtxt").innerText = "说话中..."\n\t\t\t\t\t\tmediaRecorder.start();\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmediaRecorder.stop();\n\t\t\t\t\t}\n\t\t\t\t}, 100); // 每100毫秒检测一次\n\n\t\t\t})\n\t\t\t.catch(error => {\n\t\t\t\tconsole.error("获取音频流失败:", error);\n\t\t\t});\n\t},\n')]),(0,o.eW)("\n")],-1)]))}]]),i=JSON.parse('{"path":"/posts/webaudio.html","title":"WebAudio","lang":"zh-CN","frontmatter":{"icon":"pen-to-square","date":"2024-11-26T00:00:00.000Z","category":["WebAudio"],"description":"WebAudio WebAudio 实时检测声音 代码示例：","head":[["meta",{"property":"og:url","content":"https://mister-hope.github.io/bolg/posts/webaudio.html"}],["meta",{"property":"og:site_name","content":"YFS"}],["meta",{"property":"og:title","content":"WebAudio"}],["meta",{"property":"og:description","content":"WebAudio WebAudio 实时检测声音 代码示例："}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-11-26T09:27:46.000Z"}],["meta",{"property":"article:published_time","content":"2024-11-26T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-26T09:27:46.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"WebAudio\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-11-26T00:00:00.000Z\\",\\"dateModified\\":\\"2024-11-26T09:27:46.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Mr.YFS\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"WebAudio 实时检测声音","slug":"webaudio-实时检测声音","link":"#webaudio-实时检测声音","children":[]}],"git":{"createdTime":1732613266000,"updatedTime":1732613266000,"contributors":[{"name":"youfusheng","email":"you9026131008@163.com","commits":1}]},"readingTime":{"minutes":2.02,"words":605},"filePathRelative":"posts/webaudio.md","localizedDate":"2024年11月26日","excerpt":"\\n<h2>WebAudio 实时检测声音</h2>\\n<div><br><br>代码示例：</div>\\n<pre>    detection() {\\n\\t\\t\\tvar that = this\\n\\t\\t\\t// 获取音频流\\n\\t\\t\\tnavigator.mediaDevices.getUserMedia({\\n\\t\\t\\t\\t\\taudio: true\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t.then(stream =&gt; {\\n\\t\\t\\t\\t\\tconst audioContext = new AudioContext();\\n\\t\\t\\t\\t\\tconst audioSource = audioContext.createMediaStreamSource(stream);\\n\\t\\t\\t\\t\\tconst mediaRecorder = new MediaRecorder(stream);\\n<pre><code>\\t\\t\\t\\tmediaRecorder.mimeType = \\"audio/wav\\";\\n\\t\\t\\t\\t// 创建一个BiquadFilterNode，用于降低噪声\\n\\t\\t\\t\\tlet lowPassFilter = audioContext.createBiquadFilter();\\n\\t\\t\\t\\tlowPassFilter.type = \'lowpass\'; // 设置为低通滤波器\\n\\t\\t\\t\\tlowPassFilter.frequency.value = 3000; // 设置滤波器滤除的频率范围\\n\\t\\t\\t\\t// 定义一个数组来存储音频数据块\\n\\t\\t\\t\\tvar audioChunks = [];\\n\\t\\t\\t\\t// 创建分析器节点\\n\\t\\t\\t\\tconst analyzerNode = audioContext.createAnalyser();\\n\\t\\t\\t\\tanalyzerNode.fftSize = 2048;\\n\\t\\t\\t\\t// 建立连接\\n\\t\\t\\t\\taudioSource.connect(lowPassFilter);\\n\\t\\t\\t\\t//连接扬声器（需要关闭不然会有回放）\\n\\t\\t\\t\\t// lowPassFilter.connect(audioContext.destination);\\n\\t\\t\\t\\t// 连接音频源到分析器节点\\n\\t\\t\\t\\taudioSource.connect(analyzerNode);\\n\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\tmediaRecorder.addEventListener(\'dataavailable\', function(event) {\\n\\t\\t\\t\\t\\t// 这里是录音的数据Blob对象，可以发送给后端\\n\\t\\t\\t\\t\\taudioChunks.push(event.data);\\n\\t\\t\\t\\t\\t// 使用WebSocket、Fetch API或其他方式发送audioBlob给后端\\n\\t\\t\\t\\t});\\n\\t\\t\\t\\t// 停止录音后，合并所有Blob对象\\n\\t\\t\\t\\tmediaRecorder.addEventListener(\'stop\', function() {\\n\\t\\t\\t\\t\\tif (that.demostate == true) return\\n\\t\\t\\t\\t\\tvar combinedBlob = new Blob(audioChunks, {\\n\\t\\t\\t\\t\\t\\ttype: \'audio/wav\'\\n\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\taudioChunks = []\\n\\n\\t\\t\\t\\t\\tconsole.log(combinedBlob, \\"combinedBlob\\")\\n\\t\\t\\t\\t\\tvar formData = new FormData()\\n\\t\\t\\t\\t\\t// 此处获取到blob对象后需要设置fileName满足当前项目上传需求，其它项目可直接传把blob作为file塞入formData\\n\\t\\t\\t\\t\\tvar fileOfBlob = new File([combinedBlob], new Date().getTime() + \'.wav\')\\n\\n\\t\\t\\t\\t\\tconsole.log(fileOfBlob, \\"fileOfBlob\\")\\n\\t\\t\\t\\t\\tformData.append(\'file\', fileOfBlob)\\n\\t\\t\\t\\t\\tconsole.log(formData, \\"formData\\")\\n\\t\\t\\t\\t\\tdocument.getElementById(\\"loadtxt\\").innerText = \\"思考中...\\"\\n\\t\\t\\t\\t\\t// 直接用ajax上传\\n\\t\\t\\t\\t\\tvar xhr = new XMLHttpRequest();\\n\\t\\t\\t\\t\\txhr.open(\\"POST\\", app.voicebase +\\n\\t\\t\\t\\t\\t\\"getWordfromWav\\"); /\\n\\t\\t\\t\\t\\txhr.onreadystatechange = function() {\\n\\t\\t\\t\\t\\t\\tconsole.log(xhr, \\"响应数据\\")\\n\\t\\t\\t\\t\\t\\tif (xhr.readyState == 4) {\\n\\t\\t\\t\\t\\t\\t\\tlet responseText = JSON.parse(xhr.responseText);\\n\\t\\t\\t\\t\\t\\t\\tdocument.getElementById(\\"loadtxt\\").innerText = \\"\\"\\n\\n\\t\\t\\t\\t\\t\\t\\tif (responseText.voice_text.result[0] != \\"\\") {\\n\\t\\t\\t\\t\\t\\t\\t\\tdocument.getElementById(\\"loadtxt\\").innerText = responseText\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t.voice_text.result[0] || \'\'\\n\\t\\t\\t\\t\\t\\t\\t\\tthat.问题 = responseText.voice_text.result[0]\\n\\t\\t\\t\\t\\t\\t\\t\\t提交()\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\txhr.send(formData);\\n\\t\\t\\t\\t\\txhr.onerror = function() {\\n\\t\\t\\t\\t\\t\\talert(\\"上传失败\\");\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t});\\n\\t\\t\\t\\t// 定义检测函数\\n\\t\\t\\t\\tfunction detectSpeech() {\\n\\t\\t\\t\\t\\tconst bufferLength = analyzerNode.frequencyBinCount;\\n\\t\\t\\t\\t\\tconst dataArray = new Uint8Array(bufferLength);\\n\\n\\t\\t\\t\\t\\tanalyzerNode.getByteFrequencyData(dataArray);\\n\\n\\t\\t\\t\\t\\t// 检查音频数据中是否有能量值大于阈值的部分\\n\\t\\t\\t\\t\\tconst threshold = 120; // 调整阈值以适应实际情况\\n\\t\\t\\t\\t\\tvar state = false\\n\\t\\t\\t\\t\\tfor (let i = 0; i &lt; bufferLength; i++) {\\n\\t\\t\\t\\t\\t\\tif (dataArray[i] &gt; threshold) {\\n\\t\\t\\t\\t\\t\\t\\tconsole.log(\\"有人在说话！\\");\\n\\t\\t\\t\\t\\t\\t\\tstate = true\\n\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\tif (state) {\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\treturn false\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t// 设置一个定时器，用于检测是否有人说话\\n\\t\\t\\t\\tsetInterval(() =&gt; {\\n\\t\\t\\t\\t\\t// 在这里可以添加检测声音的逻辑，例如使用Web Audio API分析音频数据\\n\\t\\t\\t\\t\\t// 如果检测到有人说话，则继续录音；否则，停止录音并发送数据给后端\\n\\t\\t\\t\\t\\tconst isSpeaking = detectSpeech(); // 自定义函数，返回true表示有人在说话，false表示没有人在说话\\n\\t\\t\\t\\t\\tif (isSpeaking) {\\n\\t\\t\\t\\t\\t\\t// 继续录音\\n\\t\\t\\t\\t\\t\\tif (that.demostate == true) {\\n\\t\\t\\t\\t\\t\\t\\tdocument.getElementById(\\"loadtxt\\").innerText = \\"\\"\\n\\t\\t\\t\\t\\t\\t\\tmediaRecorder.stop();\\n\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tdocument.getElementById(\\"loadtxt\\").innerText = \\"说话中...\\"\\n\\t\\t\\t\\t\\t\\tmediaRecorder.start();\\n\\t\\t\\t\\t\\t} else {\\n\\t\\t\\t\\t\\t\\tmediaRecorder.stop();\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}, 100); // 每100毫秒检测一次\\n\\n\\t\\t\\t})\\n\\t\\t\\t.catch(error =&gt; {\\n\\t\\t\\t\\tconsole.error(\\"获取音频流失败:\\", error);\\n\\t\\t\\t});\\n\\t},\\n</code></pre>\\n</pre>","autoDesc":true}')}}]);